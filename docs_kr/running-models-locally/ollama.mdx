---
title: "Ollama"
description: "Cline에서 로컬 AI 모델 실행을 위해 Ollama를 설정하는 빠른 가이드입니다."
---

### 사전 준비

-   Windows, macOS, 또는 Linux 컴퓨터
-   VS Code에 Cline 설치

### 설정 단계

#### 1. Ollama 설치

-   [ollama.com](https://ollama.com) 방문
-   운영체제에 맞는 버전 다운로드 및 설치

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/image%20(2)%20(1)%20(1).png"
		alt="Ollama 다운로드 페이지"
	/>
</Frame>

#### 2. 모델 선택 및 다운로드

-   [ollama.com/search](https://ollama.com/search)에서 모델 탐색
-   모델을 선택하고 명령 복사:

    ```bash
    ollama run [model-name]
    ```

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/ollama-model-grab%20(2).gif"
		alt="Ollama에서 모델 선택"
	/>
</Frame>

-   터미널을 열고 명령 실행:

    -   예시:

        ```bash
        ollama run llama2
        ```

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/starting-ollama-terminal%20(2).gif"
		alt="터미널에서 Ollama 실행"
	/>
</Frame>

이제 모델을 Cline에서 사용할 준비가 완료되었습니다.

#### 3. Cline 설정

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/ollama-setup.gif"
		alt="Ollama 전체 설정 과정"
	/>
</Frame>

VS Code를 열고 Cline을 설정합니다:

1. Cline 설정 아이콘 클릭
2. API provider로 "Ollama" 선택
3. Base URL: `http://localhost:11434/` (기본값, 보통 변경 불필요)
4. 드롭다운에서 모델 선택

### 추천 모델

Cline에 가장 적합한 모델은 **Qwen 2.5 Coder 32B**입니다. 로컬 개발에서 강력한 코딩 성능과 안정적인 도구 사용을 제공합니다.

다운로드 방법:
```bash
ollama pull qwen2.5-coder:32b
```

다른 추천 모델:
- `mistral-small:latest` - 성능과 속도의 균형이 좋음
- `codellama:34b-code` - 코딩 작업에 최적화

### 중요 참고사항

-   Cline과 함께 사용하기 전에 Ollama를 시작하세요
-   Ollama를 백그라운드에서 실행 상태로 유지하세요
-   첫 모델 다운로드는 크기에 따라 수 분이 걸릴 수 있습니다

### Compact Prompts 활성화

로컬 모델에서 더 나은 성능을 위해 Cline 설정에서 compact prompts를 활성화하세요. 핵심 기능을 유지하면서 프롬프트 크기를 90% 줄입니다.

Cline Settings → Features → Use Compact Prompt로 이동해 토글을 켭니다.

### 문제 해결

Cline이 Ollama에 연결하지 못하는 경우:

1. Ollama가 실행 중인지 확인
2. Base URL이 올바른지 확인
3. 모델이 다운로드되었는지 확인

더 자세한 정보는 [Ollama Docs](https://github.com/ollama/ollama/blob/main/docs/api.md)를 참고하세요.
